{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IylWiPWFOt95"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22699,
     "status": "ok",
     "timestamp": 1620723890140,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "cUE4n6eB-_Vd",
    "outputId": "e409c1db-6db4-4882-faa8-60eb093e7739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95884,
     "status": "ok",
     "timestamp": 1620723990421,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "4VcyfILlHOXR",
    "outputId": "b9e587bc-4883-414f-c306-740d0de38942"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 12.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 20kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 30kB 10.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 40kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 51kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 61kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 71kB 5.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 81kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 92kB 6.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 102kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 112kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 122kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 4.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Uninstalling spacy-2.2.4:\n",
      "  Would remove:\n",
      "    /usr/local/bin/spacy\n",
      "    /usr/local/lib/python3.7/dist-packages/bin/*\n",
      "    /usr/local/lib/python3.7/dist-packages/spacy-2.2.4.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/spacy/*\n",
      "  Would not remove (might be manually added):\n",
      "    /usr/local/lib/python3.7/dist-packages/bin/theano_cache.py\n",
      "    /usr/local/lib/python3.7/dist-packages/bin/theano_nose.py\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled spacy-2.2.4\n",
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 256kB/s \n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 16.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 33.7MB/s \n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 26.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 42.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=f87dfedb39945a58e0b5d26e8978f82fb11c5576455176f0c561c463f5b7e01d\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: typer, smart-open, pathy, spacy-legacy, pydantic, catalogue, srsly, thinc, spacy\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n",
      "2021-05-11 09:05:29.047393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.0.6                         \n",
      "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
      "Platform         Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\n",
      "Python version   3.7.10                        \n",
      "Pipelines                                      \n",
      "\n",
      "2021-05-11 09:05:34.071295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting en-core-web-trf==3.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7MB)\n",
      "\u001b[K     |████████████████████████████████| 459.7MB 30kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.0.0) (3.0.6)\n",
      "Collecting spacy-transformers<1.1.0,>=1.0.0rc4\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/c5/a156f9c979cc14f5f41cf2e6ecfc55d1128ac0363930ec7cc6fe4d98b4a2/spacy_transformers-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (56.1.0)\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.8.1+cu101)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/86/a6786d24d1d8f3a6cff2c60b55a7e845725a94919cd94d270ea49d82e59b/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998kB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 7.0MB/s \n",
      "\u001b[?25hCollecting transformers<4.6.0,>=3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-trf==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 31.6MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 35.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.0rc4->en-core-web-trf==3.0.0) (1.0.1)\n",
      "Installing collected packages: spacy-alignments, sacremoses, tokenizers, transformers, spacy-transformers, en-core-web-trf\n",
      "Successfully installed en-core-web-trf-3.0.0 sacremoses-0.0.45 spacy-alignments-0.8.3 spacy-transformers-1.0.2 tokenizers-0.10.2 transformers-4.5.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip uninstall spacy\n",
    "!pip install spacy\n",
    "!python -m spacy info\n",
    "# redemarrer le runtime apres\n",
    "\n",
    "!spacy download en_core_web_trf\n",
    "# !spacy download en_core_web_lg\n",
    "\n",
    "# spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "#### disable useless spacy components for performance \n",
    "#### https://spacy.io/usage/processing-pipelines#disabling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_trf') # , disable=[\"\"]\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60971,
     "status": "ok",
     "timestamp": 1620724258258,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "kbXSC5H2sBuR",
    "outputId": "efa21a58-bba6-41b2-ced2-934adb4ea15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11 09:10:01.070696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting en-core-web-lg==3.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8MB)\n",
      "\u001b[K     |████████████████████████████████| 778.8MB 18kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (56.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_lg\n",
    "sp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1620723726758,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "jP-79HmXpoc0",
    "outputId": "0a5bbbee-71e3-4bfe-ab9f-c1bd3cb80b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.2.4                         \n",
      "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
      "Platform         Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic\n",
      "Python version   3.7.10                        \n",
      "Models           en                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1619670708439,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "YMN9YgrzzkUN",
    "outputId": "e26d0bd5-4047-4515-c4c6-05c83d87a376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9585951368995221154, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14674281152\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10352637737517138979\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P832aYBrHoU7"
   },
   "source": [
    "# Load Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc0epdEIL-5t"
   },
   "source": [
    "## Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki7tVIB3HSak"
   },
   "outputs": [],
   "source": [
    "# wdir=\"/content/drive/MyDrive/TFE/Code/steam/\"\n",
    "\n",
    "# full_df = pd.read_csv(wdir+\"train.csv\") \n",
    "# # Preview the first 5 lines of the loaded data \n",
    "# full_df.head()\n",
    "# full_df['game_title']=full_df['title']\n",
    "# full_df['review_id']=full_df['review_id']\n",
    "# full_df['review_text']=full_df['user_review']\n",
    "# full_df['score']=full_df['user_suggestion']\n",
    "# full_df=full_df[[\"review_id\", \"game_title\", \"review_text\",\"score\"]]\n",
    "# full_df.groupby('game_title').review_id.nunique()\n",
    "# used_games=[\"Eternal Card Game\",\n",
    "# \"Fractured Spacy\",\n",
    "# \"Heroes & Generals\",\n",
    "# \"Robocraft\",\n",
    "# \"Bless Online\",\n",
    "# \"SMITE®\"]\n",
    "# used_games_dict = {i: used_games[i] for i in range(0, len(used_games))}  \n",
    "# print(used_games_dict)   \n",
    "# df=full_df\n",
    "# df=full_df.loc[df['game_title'].isin(used_games)]\n",
    "# print(\"df shape: \", df.shape)\n",
    "# df.to_pickle(wdir+\"1_basic_df.pkl\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5vfZR0zMBbm"
   },
   "source": [
    "## metacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qQTgqOsMGdp"
   },
   "outputs": [],
   "source": [
    "# wdir=\"/content/drive/MyDrive/TFE/Code/metacritic/\"\n",
    "\n",
    "# full_df = pd.read_csv(wdir+\"metacritic_critic_reviews.csv\") \n",
    "# # Preview the first 5 lines of the loaded data \n",
    "# full_df.head()\n",
    "# full_df=full_df.drop(['name', 'date'], axis=1)\n",
    "# full_df = full_df[full_df.platform == \"PC\"]\n",
    "# full_df=full_df.drop(['platform'], axis=1)\n",
    "# full_df = full_df.sample(frac=1).reset_index(drop=False) # random sampling and resetting index\n",
    "# full_df['game_title']=full_df['game']\n",
    "# full_df['review_id']=full_df['index']\n",
    "# full_df['review_text']=full_df['review']\n",
    "\n",
    "# full_df=full_df[[\"review_id\", \"game_title\", \"review_text\",\"score\"]]\n",
    "\n",
    "# used_games=[\"Company of Heroes 2\",      ## games with most reviews ( around 100 each ) in the dataset     \n",
    "# \"Diablo III\",                     \n",
    "# \"Sid Meier's Civilization VI\",   \n",
    "# \"SUPERHOT\",                       \n",
    "# \"XCOM 2\"]       \n",
    "# used_games_dict = {i: used_games[i] for i in range(0, len(used_games))}  \n",
    "# print(used_games_dict)            \n",
    "# df=full_df\n",
    "# df=df.loc[df['game_title'].isin(used_games)].reset_index(drop=True)\n",
    "# print(\"df shape: \", df.shape)\n",
    "# df.to_pickle(wdir+\"1_basic_df.pkl\")\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx_mDHNIisK4"
   },
   "source": [
    "## AM Videogames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 1818,
     "status": "ok",
     "timestamp": 1620724905130,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "hBUYEncZsf64",
    "outputId": "b9047f62-64e1-4ccd-af50-9405d39e443a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'diablo III', 1: 'God of War III', 2: 'Mario Kart', 3: 'The Last of Us', 4: 'StarCraft II: Wings of Liberty'}\n",
      "df shape:  (4677, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>game_title</th>\n",
       "      <th>summary</th>\n",
       "      <th>review_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>My 4 1/2 year old LOVES this</td>\n",
       "      <td>I would have thought that he was too young to ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126176</td>\n",
       "      <td>God of War III</td>\n",
       "      <td>Much better than GOW I and II</td>\n",
       "      <td>This game is a perfect squeal/\"season finale\"....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149896</td>\n",
       "      <td>StarCraft II: Wings of Liberty</td>\n",
       "      <td>An Epic Sequel to an Epic Game!</td>\n",
       "      <td>In a nutshell, this game is more or less of th...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125482</td>\n",
       "      <td>StarCraft II: Wings of Liberty</td>\n",
       "      <td>Looks nice, but who cares.</td>\n",
       "      <td>Blizzard is now my enemy. The great quality of...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126316</td>\n",
       "      <td>God of War III</td>\n",
       "      <td>Great Game ... but Expected More From the Finale</td>\n",
       "      <td>This is an descent experience but not quite wo...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  ... score\n",
       "0     123125  ...   5.0\n",
       "1     126176  ...   4.0\n",
       "2     149896  ...   5.0\n",
       "3     125482  ...   2.0\n",
       "4     126316  ...   3.0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pickle5\n",
    "# import pickle5 as pickle\n",
    "wdir=\"/content/drive/MyDrive/TFE/Code/AM_videogames/\"\n",
    "# with open(wdir+\"1_basic_df.pkl\", \"rb\") as fh:\n",
    "#   df = pickle.load(fh)\n",
    "\n",
    "df = pd.read_pickle(wdir+\"1_basic_df.pkl\")\n",
    "\n",
    "used_games=[\"diablo III\",  ## games with most reviews ( around 100 each ) in the dataset     \n",
    "\"God of War III\",     \n",
    "\"Mario Kart\",                     \n",
    "\"The Last of Us\",   \n",
    "\"StarCraft II: Wings of Liberty\"]   \n",
    "\n",
    "used_games_dict = {i: used_games[i] for i in range(0, len(used_games))}  \n",
    "print(used_games_dict)            \n",
    "\n",
    "df=df.loc[df['game_title'].isin(used_games)].reset_index(drop=True)\n",
    "print(\"df shape: \", df.shape)\n",
    "df.to_pickle(wdir+\"1_basic_df.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tN0QeJkMrVr"
   },
   "source": [
    "# data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102600,
     "status": "ok",
     "timestamp": 1619649985901,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "64JQhE-VM3WE",
    "outputId": "4a173034-0f54-486c-8ccb-b02f4c1c48fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_title\n",
      "God of War III                     905\n",
      "Mario Kart                         834\n",
      "StarCraft II: Wings of Liberty     774\n",
      "The Last of Us                     783\n",
      "diablo III                        1381\n",
      "Name: review_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('game_title').review_id.nunique())\n",
    "# df = df.sample(frac=1).reset_index(drop=True) # random sampling and resetting index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hYG_M380xAS"
   },
   "source": [
    "# spacy extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385042,
     "status": "ok",
     "timestamp": 1620722821719,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "A36078diam1H",
    "outputId": "98924002-643b-43e3-e4d8-54f31cae77a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n",
      "0    % processed\n",
      "1    % processed\n",
      "1    % processed\n",
      "2    % processed\n",
      "3    % processed\n",
      "4    % processed\n",
      "5    % processed\n",
      "6    % processed\n",
      "7    % processed\n",
      "8    % processed\n",
      "9    % processed\n",
      "9    % processed\n",
      "10    % processed\n",
      "11    % processed\n",
      "12    % processed\n",
      "13    % processed\n",
      "14    % processed\n",
      "15    % processed\n",
      "16    % processed\n",
      "17    % processed\n",
      "18    % processed\n",
      "18    % processed\n",
      "19    % processed\n",
      "20    % processed\n",
      "21    % processed\n",
      "22    % processed\n",
      "23    % processed\n",
      "24    % processed\n",
      "25    % processed\n",
      "26    % processed\n",
      "26    % processed\n",
      "27    % processed\n",
      "28    % processed\n",
      "29    % processed\n",
      "30    % processed\n",
      "31    % processed\n",
      "32    % processed\n",
      "33    % processed\n",
      "34    % processed\n",
      "34    % processed\n",
      "35    % processed\n",
      "36    % processed\n",
      "37    % processed\n",
      "38    % processed\n",
      "39    % processed\n",
      "40    % processed\n",
      "41    % processed\n",
      "42    % processed\n",
      "43    % processed\n",
      "43    % processed\n",
      "44    % processed\n",
      "45    % processed\n",
      "46    % processed\n",
      "47    % processed\n",
      "48    % processed\n",
      "49    % processed\n",
      "50    % processed\n",
      "51    % processed\n",
      "51    % processed\n",
      "52    % processed\n",
      "53    % processed\n",
      "54    % processed\n",
      "55    % processed\n",
      "56    % processed\n",
      "57    % processed\n",
      "58    % processed\n",
      "59    % processed\n",
      "60    % processed\n",
      "60    % processed\n",
      "61    % processed\n",
      "62    % processed\n",
      "63    % processed\n",
      "64    % processed\n",
      "65    % processed\n",
      "66    % processed\n",
      "67    % processed\n",
      "68    % processed\n",
      "68    % processed\n",
      "69    % processed\n",
      "70    % processed\n",
      "71    % processed\n",
      "72    % processed\n",
      "73    % processed\n",
      "74    % processed\n",
      "75    % processed\n",
      "76    % processed\n",
      "77    % processed\n",
      "77    % processed\n",
      "78    % processed\n",
      "79    % processed\n",
      "80    % processed\n",
      "81    % processed\n",
      "82    % processed\n",
      "83    % processed\n",
      "84    % processed\n",
      "85    % processed\n",
      "85    % processed\n",
      "86    % processed\n",
      "87    % processed\n",
      "88    % processed\n",
      "89    % processed\n",
      "90    % processed\n",
      "91    % processed\n",
      "92    % processed\n",
      "93    % processed\n",
      "93    % processed\n",
      "94    % processed\n",
      "95    % processed\n",
      "96    % processed\n",
      "97    % processed\n",
      "98    % processed\n",
      "99    % processed\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# POSLemmaChoice=\"SP\" # WN TB SP\n",
    "chosen_game=\"The Last of Us\"\n",
    "# \"diablo III\"\n",
    "# \"God of War III\"   \n",
    "# \"Mario Kart\"                   \n",
    "# \"The Last of Us\"  \n",
    "# \"StarCraft II: Wings of Liberty\" \n",
    "basic_df = pd.read_pickle(wdir+\"1_basic_df.pkl\")\n",
    "basic_df=basic_df.loc[basic_df[\"game_title\"]== chosen_game]\n",
    " \n",
    "basic_df=basic_df.reset_index(drop=False)\n",
    "basic_df[\"review_id\"]=basic_df[\"index\"]\n",
    "basic_df=basic_df.drop([\"index\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_len=len(basic_df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/100)\n",
    "basic_df.head()\n",
    "i=0\n",
    "doc_list=[]\n",
    "for index, row in basic_df.iterrows():\n",
    "  # i+=1\n",
    "  # if i>1: break\n",
    "  if (index%percent)==1:\n",
    "    print(int(index/df_len*100) , \"   % processed\")\n",
    "  doc = sp(row[\"review_text\"])\n",
    "  doc_list.append(doc)\n",
    "\n",
    "''' Serialization '''\n",
    "# Serialize vocab (actually the whole NLP ojbect)\n",
    "pickle.dump(doc_list, open(wdir+chosen_game+\"_SP.pickle\", \"wb\"))\n",
    "\n",
    "# ''' Deserialization '''\n",
    "# reloaded_doc_list = pickle.load(open(wdir+chosen_game+\"_SP.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPRjnNMn6zpt"
   },
   "outputs": [],
   "source": [
    "''' Deserialization '''\n",
    "loaded_doc_list = pickle.load(open(wdir+chosen_game+\"_SP.pickle\", \"rb\"))\n",
    "print(len(loaded_doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1620661033556,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "UMMpskIB7D9v",
    "outputId": "2928af53-d67b-47cd-fd89-7de99a358e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-vjkJNhhTO9"
   },
   "source": [
    "# Preprocessing reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuBbu-isHiS0"
   },
   "source": [
    "## split reviews in sentences + compute sentence polarities\n",
    "try textblob and nltk too for polarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLJVEnqVO628"
   },
   "source": [
    "Vader Sentiment polarity ( grabs negation ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paHONvLBO-11"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "def getVaderPolarity(sentence):\n",
    "  return analyser.polarity_scores(sentence)[\"compound\"]\n",
    "\n",
    "def getVaderMatch(df):\n",
    "  df[\"vader_match\"]=np.where(  \n",
    "      ( ((df['score'] > 0.5) & (df['vader_polarity'] > 0)) |  ((df['score'] < 0.5) & (df['vader_polarity'] < 0)) ) \n",
    "        , 1, 0)\n",
    "  return df\n",
    "  \n",
    "## NLTK sentiment \n",
    "\n",
    "# textblob sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "executionInfo": {
     "elapsed": 111222,
     "status": "ok",
     "timestamp": 1619649994528,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "Ui1CEI04hVyE",
    "outputId": "1e417124-1ada-4a22-8859-c45df48501f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "4677\n",
      "0    % processed\n",
      "10    % processed\n",
      "19    % processed\n",
      "29    % processed\n",
      "39    % processed\n",
      "49    % processed\n",
      "59    % processed\n",
      "69    % processed\n",
      "79    % processed\n",
      "89    % processed\n",
      "99    % processed\n",
      "split in sentences processed in  2.366905450820923  seconds\n",
      "vader processed in  5.2729737758636475  seconds\n",
      "(47525, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>game_title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vader_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>I would have thought that he was too young to ...</td>\n",
       "      <td>0.3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>However, there are new players for him to earn...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>You do have to unlock many of the courses, but...</td>\n",
       "      <td>0.5499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>The courses are fairly intuitive, so even when...</td>\n",
       "      <td>0.6422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>A fun game for the whole family.</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  ... vader_polarity\n",
       "0     123125  ...         0.3064\n",
       "1     123125  ...         0.0000\n",
       "2     123125  ...         0.5499\n",
       "3     123125  ...         0.6422\n",
       "4     123125  ...         0.5106\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize # del\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_pickle(wdir+\"1_basic_df.pkl\")\n",
    "\n",
    "tic= time.time()\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "df_len=len(df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/10)\n",
    "sentences_dic={}\n",
    "for index, row in df.iterrows():\n",
    "  i+=1\n",
    "  # if i>200:\n",
    "  #   break\n",
    "  if (i%percent)==1:\n",
    "    print(int(i/df_len*100) , \"   % processed\")\n",
    "  sentences = nltk.tokenize.sent_tokenize(row[\"review_text\"])\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    \n",
    "    sentences_dic[j] = {'review_id' : row[\"review_id\"],\n",
    "                    'game_title' : row[\"game_title\"],\n",
    "                    'sentence': sentence}\n",
    "    j+=1\n",
    "    \n",
    "sentences_df = pd.DataFrame.from_dict(sentences_dic,orient='index')  # Creation of the dataframe   \n",
    "sentences_df = sentences_df.sort_index()  # sorting by index\n",
    "\n",
    "toc= time.time() - tic\n",
    "print(\"split in sentences processed in \" , toc, \" seconds\")\n",
    "\n",
    "tic= time.time()\n",
    "sentences_df['vader_polarity'] = sentences_df[\"sentence\"].apply(getVaderPolarity)\n",
    "toc= time.time() - tic\n",
    "print(\"vader processed in \" , toc, \" seconds\")\n",
    "\n",
    "print(sentences_df.shape)\n",
    "sentences_df.to_pickle(wdir+\"2_sentences_df.pkl\")\n",
    "\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYUgVBsstYqd"
   },
   "source": [
    "## Stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111216,
     "status": "ok",
     "timestamp": 1619649994528,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "mAXn5eustYzP",
    "outputId": "68bb4b84-a84f-46a4-aea3-9cb384039e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "865\n",
      "882\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # del\n",
    "import nltk # del\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "import re # del\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "other_stop_words=\"\"\"\n",
    "game games play playing playin played\n",
    "a\thereupon\tsix\tdifferently\n",
    "a’s\thers\tso\tdowned\n",
    "able\therself\tsome\tdowning\n",
    "about\thi\tsomebody\tdowns\n",
    "above\thim\tsomehow\tearly\n",
    "according\thimself\tsomeone\tend\n",
    "accordingly\this\tsomething\tended\n",
    "across\thither\tsometime\tending\n",
    "actually\thopefully\tsometimes\tends\n",
    "after\thow\tsomewhat\tevenly\n",
    "afterwards\thowbeit\tsomewhere\tface\n",
    "again\thowever\tsoon\tfaces\n",
    "against\ti\tsorry\tfact\n",
    "ain’t\ti’d\tspecified\tfacts\n",
    "all\ti’ll\tspecify\tfelt\n",
    "allow\ti’m\tspecifying\tfind\n",
    "allows\ti’ve\tstill\tfinds\n",
    "almost\tie\tsub\tfull\n",
    "alone\tif\tsuch\tfully\n",
    "along\tignored\tsup\tfurthered\n",
    "already\timmediate\tsure\tfurthering\n",
    "also\tin\tt\tfurthers\n",
    "although\tinasmuch\tt’s\tgave\n",
    "always\tinc\ttake\tgeneral\n",
    "am\tindeed\ttaken\tgenerally\n",
    "among\tindicate\ttell\tgive\n",
    "amongst\tindicated\ttends\n",
    "an\tindicates\tth\n",
    "and\tinner\tthan\n",
    "another\tinsofar\tthank\n",
    "any\tinstead\tthanks\n",
    "anybody\tinto\tthanx\tgroup\n",
    "anyhow\tinward\tthat\tgrouped\n",
    "anyone\tis\tthat’s\tgrouping\n",
    "anything\tisn’t\tthats\tgroups\n",
    "anyway\tit\tthe\n",
    "anyways\tit’d\ttheir\n",
    "anywhere\tit’ll\ttheirs\n",
    "apart\tit’s\tthem\n",
    "appear\tits\tthemselves\n",
    "appreciate\titself\tthen\n",
    "appropriate\tj\tthence\n",
    "are\tjust\tthere\n",
    "aren’t\tk\tthere’s\tkind\n",
    "around\tkeep\tthereafter\tknew\n",
    "as\tkeeps\tthereby\n",
    "aside\tkept\ttherefore\n",
    "ask\tknow\ttherein\tlatest\n",
    "asking\tknows\ttheres\tlets\n",
    "associated\tknown\tthereupon\tlong\n",
    "at\tl\tthese\tlonger\n",
    "available\tlast\tthey\n",
    "away\tlately\tthey’d\tmade\n",
    "awfully\tlater\tthey’ll\tmake\n",
    "b\tlatter\tthey’re\tmaking\n",
    "be\tlatterly\tthey’ve\tman\n",
    "became\tleast\tthink\n",
    "because\tless\tthird\n",
    "become\tlest\tthis\tmen\n",
    "becomes\tlet\tthorough\tmr\n",
    "becoming\tlet’s\tthoroughly\tmrs\n",
    "been\tlike\tthose\tneeded\n",
    "before\tliked\tthough\tneeding\n",
    "beforehand\tlikely\tthree\n",
    "behind\tlittle\tthrough\n",
    "being\tlook\tthroughout\tnumber\n",
    "believe\tlooking\tthru\tnumbers\n",
    "below\tlooks\tthus\n",
    "beside\tltd\tto\n",
    "besides\tm\ttogether\topen\tmainly\ttoo\topened\tmany\ttook\topening\n",
    "between\tmay\ttoward\topens\n",
    "beyond\tmaybe\ttowards\torder\n",
    "both\tme\ttried\n",
    "brief\tmean\ttries\n",
    "but\tmeanwhile\ttruly\n",
    "by\tmerely\ttry\tpart\n",
    "c\tmight\ttrying\tparted\n",
    "c’mon\tmore\ttwice\tparting\n",
    "c’s\tmoreover\ttwo\tparts\n",
    "came\tmost\tu\n",
    "can\tmostly\tun\n",
    "can’t\tmuch\tunder\tpoint\n",
    "cannot\tmust\tunfortunately\tpointed\n",
    "cant\tmy\tunless\tpointing\n",
    "cause\tmyself\tunlikely\tpoints\n",
    "causes\tn\tuntil\tpresent\n",
    "certain\tunto\tpresented\n",
    "certainly\tup\tpresenting\n",
    "changes\tnd\tupon\tpresents\n",
    "clearly\tnear\tus\n",
    "co\tnearly\tuse\n",
    "com\tused\tput\n",
    "come\tuseful\tputs\n",
    "comes\tuses\troom\n",
    "concerning\tneither\tusing\trooms\n",
    "consequently\tnever\tusually\tseconds\n",
    "consider\tnevertheless\tuucp\tsees\n",
    "considering\tv\tshow\n",
    "contain\tnext\tshowed\n",
    "containing\tnine\tvarious\tshowing\n",
    "contains\tno\tvery\tshows\n",
    "corresponding\tnobody\tvia\tside\n",
    "could\tnon\tviz\tsides\n",
    "couldn’t\tnone\tvs\n",
    "course\tnoone\tw\n",
    "currently\tnor\twant\n",
    "d\tnormally\twants\tstate\n",
    "definitely\tnot\twas\tstates\n",
    "described\tnothing\twasn’t\tthing\n",
    "despite\tnovel\tway\tthings\n",
    "did\tnow\twe\tthinks\n",
    "didn’t\tnowhere\twe’d\tthought\n",
    "different\to\twe’ll\tthoughts\n",
    "do\tobviously\twe’re\ttoday\n",
    "does\tof\twe’ve\tturn\n",
    "doesn’t\toff\twelcome\tturned\n",
    "doing\toften\twell\tturning\n",
    "don’t\toh\twent\tturns\n",
    "done\tok\twere\twanted\n",
    "down\tokay\tweren’t\twanting\n",
    "downwards\told\twhat\tways\n",
    "during\ton\twhat’s\twells\n",
    "e\tonce\twhatever\n",
    "each\tone\twhen\n",
    "edu\tones\twhence\n",
    "eg\tonly\twhenever\n",
    "eight\tonto\twhere\tyear\n",
    "either\tor\twhere’s\tyears\n",
    "else\tother\twhereafter\tyoung\n",
    "elsewhere\tothers\twhereas\tyounger\n",
    "enough\totherwise\twhereby\tyoungest\n",
    "entirely\tought\twherein\tbeings\n",
    "especially\tour\twhereupon\tbig\n",
    "et\tours\twherever\tcase\n",
    "etc\tourselves\twhether\tcases\n",
    "even\tout\twhich\tclear\n",
    "ever\toutside\twhile\tdiffer\n",
    "every\tover\twhither\thence\n",
    "everybody\toverall\twho\ther\n",
    "everyone\town\twho’s\there\n",
    "everything\tp\twhoever\there’s\n",
    "everywhere\tparticular\twhole\thereafter\n",
    "ex\tparticularly\twhom\thereby\n",
    "exactly\tper\twhose\therein\n",
    "example\tperhaps\twhy\tseven\n",
    "except\tplaced\twill\tseveral\n",
    "f\tplease\twilling\tshall\n",
    "far\tplus\twish\tshe\n",
    "few\tpossible\twith\tshould\n",
    "fifth\tpresumably\twithin\tshouldn’t\n",
    "first\tprobably\twithout\tsince\n",
    "five\tprovides\twon’t\thaven’t\n",
    "followed\tq\twonder\thaving\n",
    "following\tque\twould\the\n",
    "follows\tquite\twouldn’t\the’s\n",
    "for\tqv\tx\thello\n",
    "former\tr\ty\thelp\n",
    "formerly\trather\tyes\tself\n",
    "forth\trd\tyet\tselves\n",
    "four\tre\tyou\tsensible\n",
    "from\treally\tyou’d\tsent\n",
    "further\treasonably\tyou’ll\n",
    "furthermore\tregarding\tyou’re\n",
    "g\tregardless\tyou’ve\tasks\n",
    "get\tregards\tyour\tback\n",
    "gets\trelatively\tyours\tbacked\n",
    "getting\trespectively\tyourself\tbacking\n",
    "given\tright\tyourselves\tbacks\n",
    "gives\ts\tz\tbegan\n",
    "go\tsaid\tzero\n",
    "goes\tsame\the’d\thas\n",
    "going\tsaw\the’ll\thasn’t\n",
    "gone\tsay\thow’s\thave\n",
    "got\tsaying\tmustn’t\tseemed\n",
    "gotten\tsays\tours \tseeming\n",
    "greetings\tsecond\tshan’t\tseems\n",
    "h\tsecondly\tshe’d\tseen\n",
    "had\tsee\tshe’ll\twhy’s\n",
    "hadn’t\tseeing\tshe’s\tarea\n",
    "happens\tseem\twhen’s\tasked\"\"\"\n",
    "\n",
    "#add other stop words\n",
    "for word in other_stop_words.split():\n",
    "  stop_words.append(word)\n",
    "print(len(stop_words))\n",
    "\n",
    "#add words from game titles\n",
    "for game in used_games:\n",
    "  for word in game.split():\n",
    "    # print(word.lower())\n",
    "    stop_words.append(word.lower())\n",
    "print(len(stop_words))\n",
    "\n",
    "\n",
    "def remove_stop_words(sentence,stop_words):\n",
    "  #removes common stop words, numbers and words from game titles\n",
    "  word_list = re.sub(\"[^\\w]\", \" \",  sentence).split() # remove non alphanumerical characters and split \n",
    "  filtered_word_list = [word for word in word_list if word not in stop_words]\n",
    "  filtered_word_list = [word for word in filtered_word_list if word.isnumeric()==False]\n",
    "  \n",
    "  sentence = \" \".join(filtered_word_list)\n",
    "  return sentence\n",
    "\n",
    "\n",
    "prefixes_list=[\"Early Access Review\"]\n",
    "\n",
    "def remove_prefixes(sentence,prefixes_list):\n",
    "  for prefix in prefixes_list:\n",
    "    sentence=sentence.replace( prefix, '')\n",
    "  return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 121441,
     "status": "ok",
     "timestamp": 1619650004758,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "ehNY7ZGZ2two",
    "outputId": "6e09b10c-12cf-4a8d-dd6c-7cfca3aea44f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47525\n",
      "0    % processed\n",
      "10    % processed\n",
      "20    % processed\n",
      "29    % processed\n",
      "39    % processed\n",
      "49    % processed\n",
      "59    % processed\n",
      "69    % processed\n",
      "79    % processed\n",
      "89    % processed\n",
      "99    % processed\n",
      "processed in  9.88982343673706  seconds\n",
      "(47525, 5)\n",
      "(47525, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>game_title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>processed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>I would have thought that he was too young to ...</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>enjoy kid courses unlocked week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>However, there are new players for him to earn...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>new players earn involved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>You do have to unlock many of the courses, but...</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>unlock courses unlocked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>The courses are fairly intuitive, so even when...</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>courses fairly intuitive new completely bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123125</td>\n",
       "      <td>Mario Kart</td>\n",
       "      <td>A fun game for the whole family.</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>fun family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  ...                            processed_sentence\n",
       "0     123125  ...               enjoy kid courses unlocked week\n",
       "1     123125  ...                     new players earn involved\n",
       "2     123125  ...                       unlock courses unlocked\n",
       "3     123125  ...  courses fairly intuitive new completely bomb\n",
       "4     123125  ...                                    fun family\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = pd.read_pickle(wdir+\"2_sentences_df.pkl\")\n",
    "sentences_df.head()\n",
    "\n",
    "\n",
    "tic= time.time()\n",
    "df_len=len(sentences_df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/10)\n",
    "proc_sent_dic={}\n",
    "\n",
    "for index, row in sentences_df.iterrows():\n",
    "  # if index>200:\n",
    "  #   break\n",
    "  if (index%percent)==1:\n",
    "    print(int(index/df_len*100) , \"   % processed\")\n",
    "\n",
    "  proc_sent = remove_prefixes(row[\"sentence\"],prefixes_list)\n",
    "  proc_sent = proc_sent.lower()\n",
    "  proc_sent = remove_stop_words(proc_sent,stop_words)\n",
    "\n",
    "  proc_sent_dic[index] = {'review_id' : row[\"review_id\"],\n",
    "                  'game_title' : row[\"game_title\"],\n",
    "                  'sentence': row[\"sentence\"],\n",
    "                  \"vader_polarity\": row[\"vader_polarity\"],\n",
    "                  'processed_sentence': proc_sent}\n",
    "    \n",
    "proc_sent_df = pd.DataFrame.from_dict(proc_sent_dic,orient='index')  # Creation of the dataframe   \n",
    "proc_sent_df = proc_sent_df.sort_index()  # sorting by index\n",
    "\n",
    "toc= time.time() - tic\n",
    "print(\"processed in \" , toc, \" seconds\")\n",
    "\n",
    "print(proc_sent_df.shape)\n",
    "print(proc_sent_df.shape)\n",
    "proc_sent_df.to_pickle(wdir+\"3_proc_sent_df.pkl\")\n",
    "proc_sent_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_4jMf7zaC5g"
   },
   "source": [
    "## POS + Lemmatization\n",
    "TB and WN, 30 sec for 20k sentences\n",
    "SP is way longer, 60x longer (30 min )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNRsHcdJiF3N"
   },
   "source": [
    "### Wordnet POSLemma function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123450,
     "status": "ok",
     "timestamp": 1619650006772,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "gfoPlPR9iIcm",
    "outputId": "38307399-2384-4208-ce55-59cd37795db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('the cat be sit with the bat on the striped mat under many badly fly geese',\n",
       " [('the', None),\n",
       "  ('cat', 'n'),\n",
       "  ('is', 'v'),\n",
       "  ('sitting', 'v'),\n",
       "  ('with', None),\n",
       "  ('the', None),\n",
       "  ('bats', 'n'),\n",
       "  ('on', None),\n",
       "  ('the', None),\n",
       "  ('striped', 'a'),\n",
       "  ('mat', 'n'),\n",
       "  ('under', None),\n",
       "  ('many', 'a'),\n",
       "  ('badly', 'r'),\n",
       "  ('flying', 'v'),\n",
       "  ('geese', 'a')])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags) \"v\" \"n\" \"a\" \"r\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# simplified Wordnet Tags \n",
    "def WN_simple_tagger(nltk_tag):\n",
    "\tif nltk_tag.startswith('J'):  # \"a\"\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif nltk_tag.startswith('V'): # \"v\"\n",
    "\t\treturn wordnet.VERB\n",
    "\telif nltk_tag.startswith('N'): # \"n\"\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif nltk_tag.startswith('R'): # \"r\"\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\t\t\n",
    "\t\treturn None\n",
    "\n",
    "def WordNetPOSLemmatizer(sentence):\n",
    "  # tokenize the sentence and find the POS tag for each token\n",
    "  pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "  # print(pos_tagged)\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], WN_simple_tagger(x[1])), pos_tagged))\n",
    "  # print(wordnet_tagged)\n",
    "  lemmatized_sentence = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "    if tag is None: # if there is no available tag, append the token as is\n",
    "      lemmatized_sentence.append(word)\n",
    "    else: # else use the tag to lemmatize the token\n",
    "      lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "  lemmatized_sentence = \" \".join(lemmatized_sentence) #reform the sentence\n",
    "  # print(lemmatized_sentence)\n",
    "  return lemmatized_sentence , wordnet_tagged\n",
    "\n",
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    "WordNetPOSLemmatizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcg32DJYiIU3"
   },
   "source": [
    "### TextBlob POSLemma function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123624,
     "status": "ok",
     "timestamp": 1619650006949,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "kL5Zy1a_oYGN",
    "outputId": "e0bfd109-d2e5-4a4d-e48f-2b82d68aac00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the cat be sit with the bat on the striped mat under many badly fly goose',\n",
       " [('the', None),\n",
       "  ('cat', 'n'),\n",
       "  ('is', 'v'),\n",
       "  ('sitting', 'v'),\n",
       "  ('with', None),\n",
       "  ('the', None),\n",
       "  ('bats', 'n'),\n",
       "  ('on', None),\n",
       "  ('the', None),\n",
       "  ('striped', 'a'),\n",
       "  ('mat', 'n'),\n",
       "  ('under', None),\n",
       "  ('many', 'a'),\n",
       "  ('badly', 'r'),\n",
       "  ('flying', 'v'),\n",
       "  ('geese', 'n')])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "  \n",
    "def TextBlobPOSLemmatizer(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "    words_tags = [(word, tag_dict.get(pos_tag[0], None)) for word, pos_tag in sent.tags]   # convert to our simplified  word tag \n",
    "    lemma_list = [wd.lemmatize(tag) for wd, tag in words_tags] # Lemmatize using tag\n",
    "    lemmatized_sentence = \" \".join(lemma_list)\n",
    "    return lemmatized_sentence, words_tags\n",
    "\n",
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese.'\n",
    "TextBlobPOSLemmatizer(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ddLzjCbsy6M"
   },
   "source": [
    "### Spacy POSLemma fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123797,
     "status": "ok",
     "timestamp": 1619650007125,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "xgsrw5Y7s4Mz",
    "outputId": "a73f6ccb-1efb-4c73-9285-f465a8b9d225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('he the cat be sit with the bat on the striped mat under many badly fly goose .',\n",
       " [('him', None),\n",
       "  ('the', None),\n",
       "  ('cat', 'n'),\n",
       "  ('is', None),\n",
       "  ('sitting', 'v'),\n",
       "  ('with', None),\n",
       "  ('the', None),\n",
       "  ('bats', 'n'),\n",
       "  ('on', None),\n",
       "  ('the', None),\n",
       "  ('striped', 'a'),\n",
       "  ('mat', 'n'),\n",
       "  ('under', None),\n",
       "  ('many', 'a'),\n",
       "  ('badly', 'r'),\n",
       "  ('flying', 'v'),\n",
       "  ('geese', 'n'),\n",
       "  ('.', None)])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SpacyPOSLemmatizer(sentence):\n",
    "  sentence= sp(sentence)\n",
    "  lemmatized_sentence = \" \".join([word.lemma_ for word in sentence ])\n",
    "  tagList=[]\n",
    "  tag_dict = {\"ADJ\": 'a', \"NOUN\": 'n', \"VERB\": 'v', \"ADV\": 'r'}\n",
    "  for word in sentence:\n",
    "    tagList.append( (word.text, tag_dict.get(word.pos_, None)) )\n",
    "  return lemmatized_sentence, tagList\n",
    "sentence ='Manchester United is looking to sign a forward for $90 million'\n",
    "# sentence =nlp(u'Manchester United is looking to sign a forward for $90 million')\n",
    "\n",
    "# for word in sentence:\n",
    "    # print(word.text, word.pos_, word.lemma) # pos is key, pos_ is value for key=pos, etc \n",
    "sentence = 'him the cat is sitting with the bats on the striped mat under many badly flying geese.'\n",
    "# sentence = 'the children, the child, the wheel, the wheels'\n",
    "SpacyPOSLemmatizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwS2ImXuHL0Q"
   },
   "source": [
    "### Apply PosLemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouL4CGUUtXqH"
   },
   "outputs": [],
   "source": [
    "def POSLemmaFunction(POSLem_choice, sentence ):\n",
    "    if POSLem_choice == \"WN\": return WordNetPOSLemmatizer(sentence)\n",
    "    elif POSLem_choice== \"TB\": return TextBlobPOSLemmatizer(sentence)\n",
    "    elif POSLem_choice== \"SP\": return SpacyPOSLemmatizer(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnEXzPXnkrMR"
   },
   "outputs": [],
   "source": [
    "POSLemmaChoice=\"SP\" # WN TB SP\n",
    "proc_sent_df = pd.read_pickle(wdir+\"3_proc_sent_df.pkl\")\n",
    "\n",
    "\n",
    "proc_sent_df=proc_sent_df.loc[proc_sent_df['game_title'].isin(used_games)]\n",
    "proc_sent_df=proc_sent_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "tic=time.time()\n",
    "df_len=len(proc_sent_df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/100)\n",
    "POSLemma_sent_dic={}\n",
    "for index, row in proc_sent_df.iterrows():\n",
    "\n",
    "\n",
    "  if (index%percent)==1:\n",
    "    print(int(index/df_len*100) , \"   % processed\")\n",
    "\n",
    "  processed_sentence, POS_tags= POSLemmaFunction(POSLemmaChoice , row[\"processed_sentence\"])\n",
    "\n",
    "  POSLemma_sent_dic[index] = {'review_id' : row[\"review_id\"],\n",
    "                  'game_title' : row[\"game_title\"],\n",
    "                  'sentence': row[\"sentence\"],\n",
    "                  'vader_polarity': row[\"vader_polarity\"],\n",
    "                  'processed_sentence': processed_sentence,\n",
    "                  'tags':POS_tags}\n",
    "    \n",
    "POSLemma_sent_df = pd.DataFrame.from_dict(POSLemma_sent_dic,orient='index')  # Creation of the dataframe    \n",
    "POSLemma_sent_df = POSLemma_sent_df.sort_index()  # sorting by index\n",
    "\n",
    "POSLemma_sent_df.shape\n",
    "\n",
    "toc=time.time()-tic\n",
    "print(toc, \" seconds to process\")\n",
    "\n",
    "## Save POSLemma Dataframe\n",
    "print(POSLemma_sent_df.shape)\n",
    "POSLemma_sent_df.to_pickle(wdir+\"4_\"+POSLemmaChoice+\"_sent_df.pkl\")\n",
    "\n",
    "POSLemma_sent_df.head(5)\n",
    "# Features will be tagged 'n', ignore None, and others will be Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCbmc9dEwcMQ"
   },
   "source": [
    "# Create FSPs and compute importance, DONE\n",
    "\n",
    "Not handling features repeting in the same sentence yet ( dict(keys don't stack, could add a \"stack_count\"). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GfDP_Du-YFa"
   },
   "source": [
    "Attention weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODbxDS1X6X7"
   },
   "source": [
    "## Similarity grouping/clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93GWzZlUYAbD"
   },
   "outputs": [],
   "source": [
    "FSP_df = pd.read_pickle(wdir+\"4_\"+POSLemmaChoice+\"_sent_df.pkl\")\n",
    "\n",
    "for index, row in FSP_df.iterrows():\n",
    "  if index>5:\n",
    "    break\n",
    "  print(row[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YidIFZqb-hfb"
   },
   "source": [
    "## FSPs extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6GHUnxN6zeJ"
   },
   "outputs": [],
   "source": [
    "def FSPs_extractor(POS_tags, polarity, FSPs_game_dic):\n",
    "  F_list=[]\n",
    "  S_list=[]\n",
    "  FSPs={}\n",
    "\n",
    "  # tag features and sentiments\n",
    "  for word, tag in POS_tags:\n",
    "    if tag==\"n\":\n",
    "      # print(word, tag)\n",
    "      F_list.append(word)\n",
    "    else: # or tag==\"v\"\n",
    "      S_list.append(word)\n",
    "\n",
    "  # Count FSPs and weighted importance\n",
    "  for feature in F_list:\n",
    "    if feature not in FSPs_game_dic:\n",
    "      FSPs_game_dic[feature]={} \n",
    "\n",
    "    FSPs[feature]=S_list\n",
    "\n",
    "    for sentiment in S_list:\n",
    "      if sentiment not in FSPs_game_dic[feature]:\n",
    "        FSPs_game_dic[feature][sentiment]={}\n",
    "        FSPs_game_dic[feature][sentiment][\"count\"]=1 # frequency, number of occurence of the pair\n",
    "        FSPs_game_dic[feature][sentiment][\"importance\"]=polarity # frequency weighted with polarity of the sentence containing the occurence\n",
    "      else:\n",
    "        FSPs_game_dic[feature][sentiment][\"count\"]+=1\n",
    "        FSPs_game_dic[feature][sentiment][\"importance\"]+=polarity\n",
    "      \n",
    "\n",
    "  return FSPs, FSPs_game_dic\n",
    "  # print(F_list)\n",
    "# def product_FSPs_extractor(df):\n",
    "#   count all FSPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_XeiuUHwcTB"
   },
   "outputs": [],
   "source": [
    "# FSP_df = pd.read_pickle(wdir+\"TB_sent_df.pkl\")\n",
    "FSP_df = pd.read_pickle(wdir+\"4_\"+POSLemmaChoice+\"_sent_df.pkl\")\n",
    "\n",
    "# print(FSP_df.groupby('game_title').review_id.nunique())\n",
    "chosen_game=0\n",
    "print(used_games_dict[chosen_game])\n",
    "FSP_df=FSP_df.loc[FSP_df['game_title'] == used_games_dict[chosen_game]]\n",
    "FSP_df=FSP_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tic=time.time()\n",
    "df_len=len(FSP_df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/10)\n",
    "FSPs_sent_dic={}\n",
    "FSPs_game_dic={}\n",
    "for index, row in FSP_df.iterrows():\n",
    "  # if index>5:\n",
    "  #   break\n",
    "  if (index%percent)==1:\n",
    "    print(int(index/df_len*100) , \"   % processed\")\n",
    "\n",
    "  sent_FSPs, FSPs_game_dic = FSPs_extractor(row[\"tags\"], row[\"vader_polarity\"], FSPs_game_dic)\n",
    "  # print(FSPs_game_dic)\n",
    "  FSPs_sent_dic[index] = {'review_id' : row[\"review_id\"],\n",
    "                  'game_title' : row[\"game_title\"],\n",
    "                  'sentence': row[\"sentence\"],\n",
    "                  'vader_polarity': row[\"vader_polarity\"],\n",
    "                  'processed_sentence': row[\"processed_sentence\"],\n",
    "                  'tags': row[\"tags\"],\n",
    "                  'FSPs': sent_FSPs}\n",
    "    \n",
    "\n",
    "toc=time.time()-tic\n",
    "print(toc, \" seconds to process\")\n",
    "# FSP_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeVVbfzEXRaE"
   },
   "outputs": [],
   "source": [
    "print(used_games_dict[chosen_game])\n",
    "new_FSPs={}\n",
    "for feature, sentiments in FSPs_game_dic.items():\n",
    "  for  sentiment, scores in sentiments.items():\n",
    "    new_FSPs[feature+\"_\"+sentiment]=scores\n",
    "\n",
    "\n",
    "FSPs_df = pd.DataFrame.from_dict(new_FSPs,orient='index',columns=[\"count\", \"importance\"] )  # Creation of the dataframe \n",
    "\n",
    "\n",
    "FSPs_df[\"mean_polarity\"] = FSPs_df[\"importance\"]/FSPs_df[\"count\"]\n",
    "FSPs_df = FSPs_df.sort_values('count', ascending=False)\n",
    "maxCount = FSPs_df.iloc[0][\"count\"]\n",
    "FSPs_df = FSPs_df.loc[FSPs_df[\"count\"] > maxCount/20]\n",
    "FSPs_df = FSPs_df.sort_values('importance', ascending=False)\n",
    "print(FSPs_df.shape)\n",
    "FSPs_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEv4MBNomXEk"
   },
   "outputs": [],
   "source": [
    "FSPs_df = FSPs_df.sort_values('importance', ascending=True)\n",
    "FSPs_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P90j5Tz1sqmv"
   },
   "outputs": [],
   "source": [
    "FSPs_df = FSPs_df.sort_values('importance', ascending=True)\n",
    "FSPs_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmvb0D_jNp8R"
   },
   "source": [
    "\n",
    "### TRY and ignore neutral comments ( polarity threshold )  \n",
    "\n",
    "### Clustering and visualization  \n",
    "\n",
    "\n",
    "### Wordnet distance or Word2Vec embedding for cosine similarity and further feature grouping and sentiment grouping.\n",
    "\n",
    "### handle negation \n",
    "(no need if sentence level, sentiment polarity handles it already ) \n",
    "But what if we use dependency parsing? then we need to know if the link between words is negative or not. \n",
    "\n",
    "### output as results simply the sentiment importance towards features, without sentiment words\n",
    "\n",
    "### run spacy trf on whole sentences ( lowercase or not? ) or even whole documents(!!!! mean polarity doc ment wise has even less sense ) ? and delete stop words after(or during )/get lemma afterwards  \n",
    "\n",
    "### Relationship/relation extraction\n",
    "\n",
    "# Use sentence segmentation from spacy ( linguistic features in spacy doc ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDf7nVg4jusZ"
   },
   "source": [
    "# Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6188,
     "status": "ok",
     "timestamp": 1620726742135,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "C_CoDxTqjzTK",
    "outputId": "b8aedb89-d4ce-48ce-b86b-734448353132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "chosen_game=\"The Last of Us\"\n",
    "# chosen_game=\"diablo III\"\n",
    "''' Deserialization '''\n",
    "# doc_list\n",
    "# loaded_doc_list\n",
    "doc_list = pickle.load(open(wdir+chosen_game+\"_SP.pickle\", \"rb\"))\n",
    "print(len(doc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1620726792363,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "k88y2_hS13tK",
    "outputId": "42e6851d-cb3c-406b-8a7b-5c54f38d6b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70329\n"
     ]
    }
   ],
   "source": [
    "features_list=[]\n",
    "for doc in doc_list:\n",
    "  for token in doc: \n",
    "    # print(token.vector)\n",
    "    features_list.append(token._.trf_data)\n",
    "    \n",
    "features_np=np.array( features_list)\n",
    "print(len(features_np))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60rlZryQrFQT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=2, workers=4)\n",
    "model.wv['luxurious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1620724777671,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "AombCsbkkj3t",
    "outputId": "80423f84-67b0-4a8d-ec6b-4b11cfb435e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "text = \"The Shiba Inu is a dog that is more like a cat.\"\n",
    "doc1 = sp(text)\n",
    "text = \"The dog of the neighbour is black.\"\n",
    "doc2 = sp(text)\n",
    "loaded_doc_list= [doc1,doc2]\n",
    "# np array \n",
    "features_list=[]\n",
    "for doc in loaded_doc_list:\n",
    "  for token in doc: \n",
    "    # print(token.vector)\n",
    "    features_list.append(token.vector)\n",
    "    \n",
    "features_np=np.array( features_list)\n",
    "print(len(features_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1620725040699,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "j-tSk6uiuiWr",
    "outputId": "ff6fb3fd-3279-4ff9-b91f-e4eef22ee438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Kmeans for 2 clusters\n",
      "start computing sil\n",
      "sil - highest 0.33260375\n",
      "start computing calinski\n",
      "cal - highest 7.807129425850146\n",
      "start computing davies\n",
      "davies - lowest 1.3428017823587548\n",
      "start Kmeans for 3 clusters\n",
      "start computing sil\n",
      "sil - highest 0.35625878\n",
      "start computing calinski\n",
      "cal - highest 7.313010863356076\n",
      "start computing davies\n",
      "davies - lowest 0.8570318003618538\n",
      "start Kmeans for 4 clusters\n",
      "start computing sil\n",
      "sil - highest 0.33089912\n",
      "start computing calinski\n",
      "cal - highest 6.5366526577134\n",
      "start computing davies\n",
      "davies - lowest 0.804096540631242\n",
      "start Kmeans for 5 clusters\n",
      "start computing sil\n",
      "sil - highest 0.351673\n",
      "start computing calinski\n",
      "cal - highest 6.6202542439549905\n",
      "start computing davies\n",
      "davies - lowest 0.70210942618581\n",
      "start Kmeans for 6 clusters\n",
      "start computing sil\n",
      "sil - highest 0.37266526\n",
      "start computing calinski\n",
      "cal - highest 7.207384684147427\n",
      "start computing davies\n",
      "davies - lowest 0.6216186080784861\n",
      "time to compute : 0.2034296989440918\n"
     ]
    }
   ],
   "source": [
    "######################### train clustering model + internal indices ###########\n",
    "import os\n",
    "clusterMethod=\"kmeans\" \n",
    "# clusterMethod=\"mshifts\"\n",
    "# clusterMethod=\"agglo\" \n",
    "start=time.time()\n",
    "clusteringModel, n_clusters = clusterAlgorithm(clusterMethod, features_list=features_list)\n",
    "\n",
    "total=time.time()-start\n",
    "print(\"time to compute : \" +str(total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1620725037258,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "t6iUPSEzjbu-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "def internalIndices(labels=None, features_list=None):\n",
    "    print(\"start computing sil\")\n",
    "    sil_score = metrics.silhouette_score(features_list, labels, metric='euclidean')\n",
    "    print(\"sil - highest \" + str(sil_score))\n",
    "    print(\"start computing calinski\")\n",
    "    calinski_score = metrics.calinski_harabasz_score(features_list, labels)\n",
    "    print(\"cal - highest \" + str(calinski_score))\n",
    "    print(\"start computing davies\")\n",
    "    davies_score = metrics.davies_bouldin_score(features_list, labels)\n",
    "    print(\"davies - lowest \" + str(davies_score))\n",
    "    return list((sil_score, calinski_score, davies_score))\n",
    "\n",
    "\n",
    "def Kmeans(features_list=None, n_clusters=None):\n",
    "    kmeans_model = cluster.KMeans(n_clusters=n_clusters, random_state=0).fit(features_list)\n",
    "    labels = kmeans_model.labels_\n",
    "    sil_score, calinski_score, davies_score = internalIndices(labels=labels, features_list=features_list)\n",
    "    return kmeans_model, sil_score, calinski_score, davies_score\n",
    "\n",
    "\n",
    "def KMeansAnalysis(features_list=None, n_clusters_list=None):\n",
    "    sil_scores = []\n",
    "    calinski_scores = []\n",
    "    davies_scores = []\n",
    "    kmeans_models = []\n",
    "    for n_clusters in n_clusters_list:\n",
    "        print(\"start Kmeans for \" + str(n_clusters) + \" clusters\")\n",
    "        kmeans_model, sil_score, calinski_score, davies_score = Kmeans(features_list=features_list,\n",
    "                                                                       n_clusters=n_clusters)\n",
    "        kmeans_models.append(kmeans_model)\n",
    "        sil_scores.append(sil_score)  # highest is best\n",
    "        calinski_scores.append(calinski_score)  # highest is best\n",
    "        davies_scores.append(davies_score)  # closest to 0 is best\n",
    "    # useful code if looking for different numbers of clusters\n",
    "    max_sil = max(sil_scores)\n",
    "    max_cal = max(calinski_scores)\n",
    "    min_dav = min(davies_scores)\n",
    "    max_sil_id = sil_scores.index(max_sil)\n",
    "    max_cal_id = calinski_scores.index(max_cal)\n",
    "    min_dav_id = davies_scores.index(min_dav)\n",
    "\n",
    "    return kmeans_models, sil_scores, calinski_scores, davies_scores\n",
    "\n",
    "def count_clusters(model=None):\n",
    "    labels = model.labels_\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters = len(labels_unique)\n",
    "    return n_clusters\n",
    "\n",
    "\n",
    "def MeanShiftsAnalysis(bandwidth=None, features_list=None):\n",
    "    meanshifts_model = cluster.MeanShift(bandwidth=bandwidth).fit(features_list)\n",
    "    n_clusters=count_clusters(meanshifts_model)\n",
    "    return meanshifts_model, n_clusters\n",
    "\n",
    "\n",
    "def AgglomerativeAnalysis(features_list=None):\n",
    "    agglo_model = cluster.AgglomerativeClustering().fit(features_list)\n",
    "    n_clusters = count_clusters(agglo_model)\n",
    "    return agglo_model, n_clusters\n",
    "\n",
    "\n",
    "def clusterAlgorithm(clusterMethod=None, features_list=None):\n",
    "    if clusterMethod == \"kmeans\":\n",
    "        clusteringModels = KMeansAnalysis(features_list=features_list, n_clusters_list=[2,3,4,5,6])\n",
    "        clusteringModel = clusteringModels[0][0]\n",
    "        n_clusters = 2\n",
    "\n",
    "    elif clusterMethod == \"mshifts\":\n",
    "        bandwidth = cluster.estimate_bandwidth(features_list, quantile=0.2, n_samples=100)\n",
    "        clusteringModel, n_clusters = MeanShiftsAnalysis(bandwidth=bandwidth, features_list=features_list)\n",
    "\n",
    "    elif clusterMethod == \"agglo\":\n",
    "        clusteringModel, n_clusters = AgglomerativeAnalysis(features_list=features_list)\n",
    "\n",
    "    else:\n",
    "        print(\"uncorrect clusterMethod\")\n",
    "\n",
    "    return clusteringModel, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1620724789695,
     "user": {
      "displayName": "Burléon Basil",
      "photoUrl": "",
      "userId": "15438663208525718664"
     },
     "user_tz": -120
    },
    "id": "j_mNbyihjkSP"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "def hi():\n",
    "    print(\"hi\")\n",
    "def run_PCA(features_list=None, n_components=None):\n",
    "    time_start = time.time()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    results = pca.fit_transform(features_list)\n",
    "    # print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "    print('Total explained variation: {}'.format(sum(pca.explained_variance_ratio_)))\n",
    "    print(results.shape)\n",
    "    print('PCA done in {} seconds'.format(time.time() - time_start))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_TSNE(features_list=None, n_components=None):\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=n_components, verbose=1, perplexity=40, n_iter=300)\n",
    "    results = tsne.fit_transform(features_list)\n",
    "    print('t-SNE done in {} seconds'.format(time.time() - time_start))\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_2D(df=None, results=None, palette=None, avgpal=None, save=None, avg=None):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=results[:, 0], y=results[:, 1],\n",
    "        hue=\"labels\",\n",
    "        palette=palette,\n",
    "        data=df,\n",
    "        legend=\"full\",\n",
    "        alpha=1\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        x=avg[:, 0], y=avg[:, 1],\n",
    "        c=avgpal,\n",
    "        alpha=1,\n",
    "        marker=\"X\"\n",
    "    )\n",
    "    if save == True:\n",
    "        print(\"save not implemented yet\")\n",
    "\n",
    "\n",
    "def colors(fl=None, ml=None, label_names=None, n_split=None):\n",
    "    colorset = []\n",
    "    for label in label_names:\n",
    "        if label[0] == \"h\":\n",
    "            colorset.append(\"green\")\n",
    "        if label[0] == \"i\":\n",
    "            colorset.append(\"red\")\n",
    "        if label[0] == \"o\":\n",
    "            colorset.append(\"blue\")\n",
    "\n",
    "    if fl == \"all\":\n",
    "        flcolors = colorset\n",
    "    elif isinstance(fl, str):\n",
    "        flcolors = [fl] * len(label_names)\n",
    "        if fl in label_names:\n",
    "            flcolors = []\n",
    "            for label in label_names:\n",
    "                if label == fl:\n",
    "                    flcolors.append(\"gray\")\n",
    "                else:\n",
    "                    flcolors.append(\"white\")\n",
    "\n",
    "    if ml == \"all\":\n",
    "        mlcolors = colorset\n",
    "    elif isinstance(ml, str):\n",
    "        mlcolors = [ml] * len(label_names)\n",
    "        if fl in label_names:\n",
    "            mlcolors = []\n",
    "            for label in label_names:\n",
    "                if label == fl:\n",
    "                    mlcolors.append(\"gray\")\n",
    "                else:\n",
    "                    mlcolors.append(\"white\")\n",
    "\n",
    "    repeated_mlcolors = list(\n",
    "        itertools.chain.from_iterable(itertools.repeat(mlcolors[i], n_split[i]) for i in range(len(mlcolors))))\n",
    "    return flcolors, repeated_mlcolors\n",
    "\n",
    "\n",
    "def visualize_3D(results=None, labels=None, save=None):\n",
    "    colors = []\n",
    "    for i in labels[\"labels\"]:\n",
    "        if i[0] == \"h\":\n",
    "            colors.append(\"green\")\n",
    "        if i[0] == \"i\":\n",
    "            colors.append(\"red\")\n",
    "        if i[0] == \"o\":\n",
    "            colors.append(\"blue\")\n",
    "        if i[0] == \"P\":\n",
    "            colors.append(\"green\")\n",
    "        if i[0] == \"U\":\n",
    "            colors.append(\"red\")\n",
    "\n",
    "    ax = plt.figure(figsize=(16, 10)).gca(projection='3d')\n",
    "    ax.scatter(\n",
    "        xs=results[:, 0], ys=results[:, 1], zs=results[:, 2],\n",
    "        c=colors\n",
    "    )\n",
    "    ax.set_xlabel('dim-one')\n",
    "    ax.set_ylabel('dim-two')\n",
    "    ax.set_zlabel('dim-three')\n",
    "    plt.show()\n",
    "    if save == True:\n",
    "        print(\"save not implemented yet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRzRreGAJuZpIrglhTwVHm",
   "collapsed_sections": [
    "IylWiPWFOt95",
    "8tN0QeJkMrVr",
    "J-vjkJNhhTO9",
    "DuBbu-isHiS0",
    "YYUgVBsstYqd",
    "B_4jMf7zaC5g",
    "UNRsHcdJiF3N",
    "Lcg32DJYiIU3",
    "0ddLzjCbsy6M",
    "KwS2ImXuHL0Q"
   ],
   "name": "TRF.ipynb",
   "provenance": [
    {
     "file_id": "1ogBtFse65OWO1ZssH_l9bGhMgHOtQKX2",
     "timestamp": 1619649753373
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
