{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f76f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir=\"AM_videogames/\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "# sp_trf_vector = spacy.load('en_core_web_trf', disable[])\n",
    "# spacy.require_gpu()\n",
    "sp_trf = spacy.load('en_core_web_trf')\n",
    "\n",
    "# https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7980c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021\n",
      "Cuda compilation tools, release 11.3, V11.3.58\n",
      "Build cuda_11.3.r11.3/compiler.29745058_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeefa0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.0.6                         \n",
      "Location         C:\\Users\\basil\\miniconda3\\envs\\basic\\lib\\site-packages\\spacy\n",
      "Platform         Windows-10-10.0.19041-SP0     \n",
      "Python version   3.9.4                         \n",
      "Pipelines        en_core_web_md (3.0.0), en_core_web_trf (3.0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1866337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n",
      "593 % processed\n"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1 # number of loop\n",
    "\n",
    "\n",
    "# POSLemmaChoice=\"SP\" # WN TB SP\n",
    "wdir=\"AM_videogames/\"\n",
    "chosen_game=\"diablo III\"\n",
    "\n",
    "\n",
    "basic_df = pd.read_pickle(wdir+\"1_df.pkl\") # 2 is sentence split, with stopwords \n",
    "basic_df=basic_df.loc[basic_df[\"game_title\"]==chosen_game]\n",
    "basic_df=basic_df.reset_index(drop=True)\n",
    "# basic_df.head()\n",
    "\n",
    "\n",
    "df_len=len(basic_df.index)\n",
    "print(df_len)\n",
    "percent=int(df_len/100)\n",
    "doc_list=[]\n",
    "for index, row in basic_df.iterrows():\n",
    "    \n",
    "    if (index%percent)==1:\n",
    "        print(int(index/df_len*100) , \" % processed\", end=\"\\r\", flush=True)\n",
    "\n",
    "    doc = sp_trf(row[\"review_text\"])\n",
    "    doc_list.append(doc)\n",
    "# print(len(doc_data)) \n",
    "\n",
    "''' Serialization '''\n",
    "# Serialize vocab (actually the whole NLP ojbect)\n",
    "pickle.dump(doc_list, open(wdir+chosen_game+\"_SP_trf.pickle\", \"wb\"))\n",
    "\n",
    "''' Deserialization '''\n",
    "loaded_doc_list = pickle.load(open(wdir+chosen_game+\"_SP_trf.pickle\", \"rb\"))\n",
    "print(len(loaded_doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSLemmaChoice=\"SP\" # WN TB SP\n",
    "wdir=\"AM_videogames/\"\n",
    "chosen_game=\"diablo III\"\n",
    "chosen_game_list=[]\n",
    "used_games=[\"Company of Heroes 2\",      ## games with most reviews ( around 100 each ) in the dataset     \n",
    "\"Diablo III\",                     \n",
    "\"Sid Meier's Civilization VI\",   \n",
    "\"SUPERHOT\",                       \n",
    "\"XCOM 2\"]  \n",
    "\n",
    "basic_df = pd.read_pickle(wdir+\"1_df.pkl\") # 2 is sentence split, with stopwords \n",
    "basic_df=basic_df.loc[basic_df[\"game_title\"]==chosen_game]\n",
    "basic_df=basic_df.reset_index(drop=True)\n",
    "basic_df.head(10)\n",
    "texts=[]\n",
    "for index,row in basic_df.iterrows():\n",
    "    texts.append(row[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593  % processed % processed  % processed\n",
      "2min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1 # number of loop\n",
    "i=0\n",
    "doc_list=[]\n",
    "texts_number=len(texts)\n",
    "for doc in sp_trf.pipe(texts, batch_size=64):\n",
    "    i+=1\n",
    "    print(int(i/texts_number*100) , \" % processed\", end=\"\\r\", flush=True)\n",
    "    doc_list.append(doc)\n",
    "\n",
    "''' Serialization '''\n",
    "# Serialize vocab (actually the whole NLP ojbect)\n",
    "pickle.dump(doc_list, open(wdir+chosen_game+\"_SP_trf.pickle\", \"wb\"))\n",
    "\n",
    "''' Deserialization '''\n",
    "loaded_doc_list = pickle.load(open(wdir+chosen_game+\"_SP_trf.pickle\", \"rb\"))\n",
    "print(len(loaded_doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(1, 7, 768)\n",
      "(1, 768)\n",
      "The [[1]]\n"
     ]
    }
   ],
   "source": [
    "#https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/04_embeddings_continued.html\n",
    "# great explanations of spacy transformers objects \n",
    "\n",
    "test= sp_trf(\"The cat is black.\")\n",
    "\n",
    "print(len(test)) # 5 tokens\n",
    "    \n",
    "print(test._.trf_data.tensors[0].shape) # outputs for 7 tokens bbecause trf works with sub-words\n",
    "print(test._.trf_data.tensors[1].shape) # whole doc output\n",
    "# Check the first ten dimensions of the tensor\n",
    "\n",
    "\n",
    "# [0] to enter the batch ( only one batch in this case), then print first 10 dimensions of each token\n",
    "# print(test._.trf_data.tensors[0][0][:10] )\n",
    "token_index=0\n",
    "print(test[token_index], test._.trf_data.align[token_index].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x1a3304baea0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1a33050d400>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1a3304eef40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1a330547400>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1a330534880>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1a330554c80>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_trf.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "''' Deserialization '''\n",
    "loaded_doc_list = pickle.load(open(wdir+chosen_game+\"_SP_trf.pickle\", \"rb\"))\n",
    "print(len(loaded_doc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 140, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_doc_list[2]._.trf_data.tensors[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
